# coding: utf-8
# _This script requires Python 3_
#
# Inspecting aggregated impact outputs
#
# Using _HazImp_ for evaluating the impacts of a severe weather event,
# the code automatically aggregates on the basis of a regional areas -
# either meshblock, SA1 or SA2. Other regions could also be
# considered.
#
# Currently, the process relies on specific formats for input and
# produces specific format output.

# ### Input data

# * Forecast data is translated into GeoTIFF format from the raw
# NetCDF files generated by the ACCESS model. The _HazImp_ code
# needs to be modified to directly read netCDF format. Issues here
# could include conflicts in GDAL, NetCDF and Python libraries used
# to execute the code.

# * Exposure data is provided in a pre-defined comma-separated values
# format. _HazImp_ should be updated to enable ingestion of a
# service

# * The forecast dataset is specified in a text-based configuration
# file. It is possible to provide a list of hazard layers to the
# configuration of _HazImp_, which would enable an ensemble of
# forecast data to be ingested. This has not been tested with the
# current development branch. Alternatives could include making the
# input forecast a command line argument, or scripting an
# intermediate process to edit the configuration file for each
# ensemble member realisation.

# ### Output data

# * Output is currently a comma-separated values format, with entries
# for each region in the forecast model domain. Ideally this should
# be changed to generate a vector shapefile, or possibly webservice
# that can be ingested by end-user systems (e.g. Visual Weather can
# read shapefiles).


import os
import sys
import time
import datetime
from os.path import dirname, realpath, isdir, join as pjoin

import logging
from collections import OrderedDict

import pandas as pd
import geopandas as gpd
import numpy as np


# The columns represent the mean and total values for each region.
# * "SA1_MAIN16" = SA1 code, named to match the field name in the region file
# * "SLM" = mean structural loss
# * "SLT" = total structural loss
# * "RVM" = mean replacement value
# * "RVT" = total replacement value
# * "SLRM" = mean structural loss ratio ([0, 1])
# * "SLRT" = standard deviation of structural loss ratio


# Presently this is hard coded to match the fields of the input file
# and the expected field widths of the output. When implemented into
# HazImp, this will need to be determined automatically from the input
# zone file, and appended with the known columns that are generated
# within HazImp (currently listed just above).

SCHEMA={
  'geometry':'Polygon',
  'properties':OrderedDict([
      ('SA1_MAIN16','int:16'),
      ('SA1_7DIG16','int:16'),
      ('SA2_MAIN16','int:11'),
      ('SA2_5DIG16','int:11'),
      ('SA2_NAME16','str:50'),
      ('SA3_CODE16','int:9'),
      ('SA3_NAME16','str:50'),
      ('SA4_CODE16','int:9'),
      ('SA4_NAME16','str:50'),
      ('GCC_CODE16','str:10'),
      ('GCC_NAME16','str:50'),
      ('STE_CODE16','int:9'),
      ('STE_NAME16','str:50'),
      ('AREASQKM16','float:12.4'),
      ('Shape_Leng','float:9.6'),
      ('Shape_Area','float:9.6'),
      ('SLM','float:15.2'),
      ('SLT','float:15.2'),
      ('RVM','float:15.2'),
      ('RVT','float:15.2'),
      ('SLRM','float:9.7'),
      ('SLRT','float:9.7')
  ])
}


# pylint: disable=R0914
def flStartLog(logFile, logLevel, verbose=False, datestamp=False, newlog=True):
    """
    Start logging to logFile all messages of logLevel and higher.
    Setting ``verbose=True`` will report all messages to STDOUT as well.

    :param str logFile: Full path to log file.
    :param str logLevel: String specifiying one of the standard Python logging
                         levels ('NOTSET','DEBUG','INFO','WARNING','ERROR',
                         'CRITICAL')
    :param boolean verbose: ``True`` will echo all logging calls to STDOUT
    :param boolean datestamp: ``True`` will include a timestamp of the creation
                              time in the filename.
    :param boolean newlog: ``True`` will create a new log file each time this
                           function is called. ``False`` will append to the
                           existing file.

    :returns: :class:`logging.logger` object.

    Example: flStartLog('/home/user/log/app.log', 'INFO', verbose=True)
    """
    if datestamp:
        base, ext = os.path.splitext(logFile)
        curdate = datetime.datetime.now()
        curdatestr = curdate.strftime('%Y%m%d%H%M')
        # The lstrip on the extension is required as splitext leaves it on.
        logFile = "%s.%s.%s" % (base, curdatestr, ext.lstrip('.'))

    logDir = os.path.dirname(os.path.realpath(logFile))
    if not os.path.isdir(logDir):
        try:
            os.makedirs(logDir)
        except OSError:
            # Unable to create the directory, so stick it in the
            # current working directory:
            path, fname = os.path.split(logFile)
            logFile = os.path.join(os.getcwd(), fname)

    if newlog:
        mode = 'w'
    else:
        mode = 'a'

    logging.basicConfig(level=getattr(logging, logLevel),
                        format='%(asctime)s: %(message)s',
                        datefmt='%Y-%m-%d %H:%M:%S',
                        filename=logFile,
                        filemode=mode)
    LOGGER = logging.getLogger()

    if len(LOGGER.handlers) < 2:
        # Assume that the second handler is a StreamHandler for verbose
        # logging. This ensures we do not create multiple StreamHandler
        # instances that will *each* print to STDOUT
        if verbose:
            console = logging.StreamHandler(sys.stdout)
            console.setLevel(getattr(logging, logLevel))
            formatter = logging.Formatter('%(asctime)s: %(message)s',
                                          '%H:%M:%S', )
            console.setFormatter(formatter)
            LOGGER.addHandler(console)

    LOGGER.info('Started log file %s (detail level %s)' % (logFile, logLevel))
    LOGGER.info('Running %s (pid %d)' % (sys.argv[0], os.getpid()))
    return LOGGER

def startup():
    """
    Parse command line arguments, set up logging and attempt
    to execute the main functions.

    """
    parser = argparse.ArgumentParser()
    parser.add_argument('-c', '--config_file', help='The configuration file')
    parser.add_argument('-s', '--shapefile',
                        help='Shapefile containing regions')
    parser.add_argument('-i', '--impactfile',
                        help='File containing impact data')
    parser.add_argument('-o', '--outputfile',
                        help='Output file location')
    parser.add_argument('-l', '--loglevel',
                        help='Logging detail level')
    parser.add_argument('-v', '--verbose', help='Verbose output',
                        action='store_true')
    parser.add_argument('-d', '--debug', help='Allow pdb traces',
                        action='store_true')
    args = parser.parse_args()

    logfile = pjoin(os.getcwd(), 'mergeImpact.log')

    if args.loglevel:
        logLevel = args.loglevel
    else:
        logLevel = 'INFO'

    if args.verbose:
        verbose = True
    else: 
        verbose = False

    logger = flStartLog(logfile, logLevel, verbose, datestamp=True)

    if args.impactfile:
        impactFile = args.impactfile
        logger.info("Impact file: {0}".format(impactFile))
    else:
        logger.error("No impact data file provided")
        raise IOError("No impact data file provided")

    if args.shapefile:
        shapeFile = args.shapefile
        logger.info("Shape file: {0}".format(shapeFile))
    else:
        logger.error("No shape file provided")
        raise IOError("No shape file provided")

    if args.outputfile:
        outputFile = args.outputfile
        logger.info("Output file: {0}".format(outputFile))
        outputDir = dirname(realpath(outputFile))
        if not isdir(outputDir):
            try:
                os.makedirs(outputDir)
            except OSError:
                raise
    else:
        logger.warn("No output file specified")
        base, ext = os.path.splitext(impactFile)
        outputFile = "{0}.shp".format(base)
        logger.warn("Using default output path: {0}".format(outputFile))

    mergeImpact(impactFile, shapeFile, outputFile)
    logger.info("Completed mergeImpact.py")

def mergeImpact(impactFile, shapeFile, output, joinField='SA1_MAIN16'):

    logging.info("Merging impact data with region shape file")
    colnames = [joinField, "SLM", "SLT", "RVM", "RVT", "SLRM", "SLRT"]

    dtype = {joinField:str}
    logging.debug("Loading impact data: {0}".format(impactFile))
    df = pd.read_csv(impactFile,
                     names=colnames, dtype=dtype,
                     skiprows=3)
    logging.info(df.columns)
    logging.debug("Loading shape file: {0}".format(shapeFile))
    gdf = gpd.read_file(shapeFile)
    logging.debug("Merging on {0}".format(joinField))
    mgdf = gdf.merge(df, left_on='SA1_MAIN16', right_on=joinField)
    logging.info("Writing output file: {0}".format(output))
    try:
        mgdf.to_file(output, schema=SCHEMA)
    except:
        logging.exception("Cannot create output file")
        raise

if __name__ == "__main__":
    import argparse
    startup()

